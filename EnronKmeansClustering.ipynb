{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "cf6d230331099d4773a6137a56c3541746da970bab15c4b63349a6079cb56b25"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Performing Kmeans Clustering on Email Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The text has been tokenized an preprocessed using functions from the NLPCode.py script."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\faria\\Documents\\Graduate School Files\\Winter2021\\inf2011\\TDW PRES\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\faria\\Documents\\Graduate School Files\\Winter2021\\inf2011\\TDW PRES\\thread10body.csv\")"
   ]
  },
  {
   "source": [
    "the following list of words was used to subset the larger enron email dataset to extract emails only containing scheduling related to emails from Vince Kaminski who was a research scientist at Enron. a threshold of 3 was set; if an email contained 3 or more of these words, it was deemed to be a part of a scheduling activity.\n",
    "\n",
    "{\"schedul\", \"meet\", \"conference\",  \"appointment\", \"plan\", \"time\",  \"book\",  \"day\", \"week\",  \"remind\", \"call\", \"avail\", \"see\" ,\"soon\", \"confirm\", \"work\", \"invit\", \"tomorrow\", \"today\", \"evening\", \"morning\", \"afternoon\" }\n",
    "\n",
    "Through "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    ThreadID                                          EmailBody  \\\n",
       "0         10  Ashley.\\r\\r\\r\\n\\r\\r\\r\\nFYI. What about Oct 16?...   \n",
       "1         10  : Re: Hello from Vince Kaminski at Enron\\r\\r\\r...   \n",
       "2         10  : Re: Hello from Vince Kaminski at Enron\\r\\r\\r...   \n",
       "3         10  : Re: Hello from Vince Kaminski at Enron\\r\\r\\r...   \n",
       "4         10  : Re: Hello from Vince Kaminski at Enron\\r\\r\\r...   \n",
       "5         10  Ashley,\\r\\r\\r\\n\\r\\r\\r\\nI agree with you. Two t...   \n",
       "6         10  : Re: Hello from Vince Kaminski at Enron  \\r\\r...   \n",
       "7         10  : Re: Hello from Vince Kaminski at Enron  \\r\\r...   \n",
       "8         10  : Re: Hello from Vince Kaminski at Enron\\r\\r\\r...   \n",
       "9         10  Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nThanks for the message....   \n",
       "10        10  Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nThe date of our trip to...   \n",
       "11        10  Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nLet's see if we can eit...   \n",
       "12        10  Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nSorry for not getting b...   \n",
       "13        10  : Re: Hello from Vince Kaminski at Enron\\r\\r\\r...   \n",
       "14        10  Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nThanks for the invitati...   \n",
       "15        10  : Re: Hello from Vince Kaminski at Enron\\r\\r\\r...   \n",
       "16        10  : Re: Hello from Vince Kaminski at Enron\\r\\r\\r...   \n",
       "17        10  : Hello from Vince Kaminski at Enron\\r\\r\\r\\n> ...   \n",
       "\n",
       "                                            CleanBody  \\\n",
       "0   ['what', 'about', 'forwarded', 'by', 'on', 'on...   \n",
       "1   ['from', 'at', 'i', 'sent', 'you', 'a', 'reply...   \n",
       "2   ['from', 'at', 'our', 'seminars', 'are', 'to',...   \n",
       "3   ['from', 'at', 'that', 'will', 'be', 'a', 'gre...   \n",
       "4   ['from', 'at', 'the', 'date', 'of', 'our', 'tr...   \n",
       "5   ['i', 'agree', 'with', 'you', 'trips', 'are', ...   \n",
       "6   ['from', 'at', 'the', 'that', 'i', 'have', 'sc...   \n",
       "7   ['from', 'at', \"'s\", 'see', 'if', 'we', 'can',...   \n",
       "8   ['from', 'at', 'i', 'spoke', 'too', 'soon', 'a...   \n",
       "9   ['for', 'the', 'message', 'i', 'am', 'working'...   \n",
       "10  ['the', 'date', 'of', 'our', 'trip', 'to', 'ha...   \n",
       "11  [\"'s\", 'see', 'if', 'we', 'can', 'either', 're...   \n",
       "12  ['for', 'not', 'getting', 'back', 'to', 'you',...   \n",
       "13  ['from', 'at', 'you', 'mentioned', 'so', 'i', ...   \n",
       "14  ['for', 'the', 'invitation', 'to', 'speak', 'o...   \n",
       "15  ['from', 'at', 'send', 'me', 'a', 'title', 'fo...   \n",
       "16  ['from', 'at', 'for', 'not', 'getting', 'back'...   \n",
       "17  ['from', 'at', 'i', 'hope', 'you', 'remember',...   \n",
       "\n",
       "                                           CleanBody2  \n",
       "0                                 ['forwarded', 'cc']  \n",
       "1   ['sent', 'reply', 'earlier', 'month', 'have', ...  \n",
       "2   ['are', 'works', 'please', 'send', 'title', 'a...  \n",
       "3   ['will', 'be', 'great', 'talk', 'email', 'oren...  \n",
       "4   ['date', 'trip', 'has', 'been', 'set', 'will',...  \n",
       "5   ['agree', 'are', 'best', 'solution', 'course',...  \n",
       "6   ['have', 'scheduled', 'diningmeeting', 'room',...  \n",
       "7   ['see', 'can', 'rearrange', 'seminar', 'change...  \n",
       "8   ['spoke', 'too', 'soon', 'apparently', 'semina...  \n",
       "9   ['message', 'am', 'working', 'recruiter', 'fin...  \n",
       "10  ['date', 'trip', 'has', 'been', 'set', 'will',...  \n",
       "11  ['see', 'can', 'rearrange', 'seminar', 'change...  \n",
       "12  ['not', 'getting', 'back', 'earlier', 'is', 's...  \n",
       "13  ['mentioned', 'so', 'reserved', 'week', 'is', ...  \n",
       "14  ['invitation', 'speak', 'split', 'presentation...  \n",
       "15  ['send', 'title', 'talk', 'title', 'etc', 'abs...  \n",
       "16  ['not', 'getting', 'back', 'earlier', 'is', 's...  \n",
       "17  ['hope', 'remember', 'visited', 'together', 'g...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ThreadID</th>\n      <th>EmailBody</th>\n      <th>CleanBody</th>\n      <th>CleanBody2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>Ashley.\\r\\r\\r\\n\\r\\r\\r\\nFYI. What about Oct 16?...</td>\n      <td>['what', 'about', 'forwarded', 'by', 'on', 'on...</td>\n      <td>['forwarded', 'cc']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron\\r\\r\\r...</td>\n      <td>['from', 'at', 'i', 'sent', 'you', 'a', 'reply...</td>\n      <td>['sent', 'reply', 'earlier', 'month', 'have', ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron\\r\\r\\r...</td>\n      <td>['from', 'at', 'our', 'seminars', 'are', 'to',...</td>\n      <td>['are', 'works', 'please', 'send', 'title', 'a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron\\r\\r\\r...</td>\n      <td>['from', 'at', 'that', 'will', 'be', 'a', 'gre...</td>\n      <td>['will', 'be', 'great', 'talk', 'email', 'oren...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron\\r\\r\\r...</td>\n      <td>['from', 'at', 'the', 'date', 'of', 'our', 'tr...</td>\n      <td>['date', 'trip', 'has', 'been', 'set', 'will',...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10</td>\n      <td>Ashley,\\r\\r\\r\\n\\r\\r\\r\\nI agree with you. Two t...</td>\n      <td>['i', 'agree', 'with', 'you', 'trips', 'are', ...</td>\n      <td>['agree', 'are', 'best', 'solution', 'course',...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron  \\r\\r...</td>\n      <td>['from', 'at', 'the', 'that', 'i', 'have', 'sc...</td>\n      <td>['have', 'scheduled', 'diningmeeting', 'room',...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron  \\r\\r...</td>\n      <td>['from', 'at', \"'s\", 'see', 'if', 'we', 'can',...</td>\n      <td>['see', 'can', 'rearrange', 'seminar', 'change...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron\\r\\r\\r...</td>\n      <td>['from', 'at', 'i', 'spoke', 'too', 'soon', 'a...</td>\n      <td>['spoke', 'too', 'soon', 'apparently', 'semina...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nThanks for the message....</td>\n      <td>['for', 'the', 'message', 'i', 'am', 'working'...</td>\n      <td>['message', 'am', 'working', 'recruiter', 'fin...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nThe date of our trip to...</td>\n      <td>['the', 'date', 'of', 'our', 'trip', 'to', 'ha...</td>\n      <td>['date', 'trip', 'has', 'been', 'set', 'will',...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10</td>\n      <td>Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nLet's see if we can eit...</td>\n      <td>[\"'s\", 'see', 'if', 'we', 'can', 'either', 're...</td>\n      <td>['see', 'can', 'rearrange', 'seminar', 'change...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>10</td>\n      <td>Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nSorry for not getting b...</td>\n      <td>['for', 'not', 'getting', 'back', 'to', 'you',...</td>\n      <td>['not', 'getting', 'back', 'earlier', 'is', 's...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron\\r\\r\\r...</td>\n      <td>['from', 'at', 'you', 'mentioned', 'so', 'i', ...</td>\n      <td>['mentioned', 'so', 'reserved', 'week', 'is', ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>10</td>\n      <td>Shmuel,\\r\\r\\r\\n\\r\\r\\r\\nThanks for the invitati...</td>\n      <td>['for', 'the', 'invitation', 'to', 'speak', 'o...</td>\n      <td>['invitation', 'speak', 'split', 'presentation...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron\\r\\r\\r...</td>\n      <td>['from', 'at', 'send', 'me', 'a', 'title', 'fo...</td>\n      <td>['send', 'title', 'talk', 'title', 'etc', 'abs...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>10</td>\n      <td>: Re: Hello from Vince Kaminski at Enron\\r\\r\\r...</td>\n      <td>['from', 'at', 'for', 'not', 'getting', 'back'...</td>\n      <td>['not', 'getting', 'back', 'earlier', 'is', 's...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>10</td>\n      <td>: Hello from Vince Kaminski at Enron\\r\\r\\r\\n&gt; ...</td>\n      <td>['from', 'at', 'i', 'hope', 'you', 'remember',...</td>\n      <td>['hope', 'remember', 'visited', 'together', 'g...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "source": [
    "'CleanBody2' column contains the chunking of specific POS tags.\n",
    "- patternV3 = 'VP: {<VB.?>*<MD.?>*<NN>*<JJ.?>*<RB.?>*}' #all forms of verbs, nouns, adjectives and adverbs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "email 0\n['forwarded', 'cc']\nemail 1\n['sent', 'reply', 'earlier', 'month', 'have', \"n't\", 'heard', 'date', 'department', 'has', 'can', 'schedule', 'visit', 'would', 'like', 'invite', 'give', 'seminar', 'will', 'be', 'attended', 'many', 'graduate', 'faculty', 'will', 'give', 'opportunity', 'tell', 'program', 'sufficient', 'leadtime', 'can', 'advertise', 'seminar', 'school', 'financial', 'engineering', 'email', 'oren', 'phone', 'original', 'message']\nemail 2\n['are', 'works', 'please', 'send', 'title', 'abstract', 'email', 'oren', 'phone', 'original', 'message']\nemail 3\n['will', 'be', 'great', 'talk', 'email', 'oren', 'phone', 'original', 'message', 'pm']\nemail 4\n['date', 'trip', 'has', 'been', 'set', 'will', 'be', 'shall', 'be', 'glad', 'make', 'presentation', 'energy', 'development', 'valuation', 'role', 'developing', 'forward', 'natural', 'gas', 'electricity', 'let', 'know', 'topic', 'would', 'be', 'interest', 'is', 'case', 'shall', 'follow', 'title', 'abstract', 'way', 'are', 'free', 'dinner', 'cc']\nemail 5\n['agree', 'are', 'best', 'solution', 'course', 'rearranges', 'speaker', 'list', 'will', 'work', 'presentation', 'as', 'well', 'so', 'many', 'different', 'is', 'never', 'easy', 'patience', 'cc']\nemail 6\n['have', 'scheduled', 'diningmeeting', 'room', 'campus', 'is', 'set', 'pm', 'pm', 'can', 'attempt', 'change', 'presentation', 'date', 'again', 'however', 'feel', 'would', 'be', 'very', 'unlikely', 'are', 'able', 'get', 'facility', 'is', 'campus', 'point', 'may', 'be', 'better', 'plan', 'making', 'seperate', 'so', 'are', 'able', 'handle', 'maybe', 'could', 'combine', 'presentation', 'same', 'visit', 'do', 'think', 'cc']\nemail 7\n['see', 'can', 'rearrange', 'seminar', 'change', 'date', 'visit', 'campus', 'coordinator', 'is', 'very', 'efficient', 'got', 'faculty', 'room', 'presentation', 'morning', 'cc']\nemail 8\n['spoke', 'too', 'soon', 'apparently', 'seminar', 'slot', 'was', 'already', 'filled', 'will', 'see', 'can', 'switch', 'speaker', 'week', 'following', 'week', 'case', 'are', 'dinner', 'email', 'oren', 'phone', 'original', 'message', 'pm']\nemail 9\n['message', 'am', 'working', 'recruiter', 'finalize', 'date', 'trip', 'shall', 'shoot', 'date', 'works', 'rest', 'team', 'cc']\nemail 10\n['date', 'trip', 'has', 'been', 'set', 'will', 'be', 'shall', 'be', 'glad', 'make', 'presentation', 'energy', 'development', 'valuation', 'role', 'developing', 'forward', 'natural', 'gas', 'electricity', 'let', 'know', 'topic', 'would', 'be', 'interest', 'is', 'case', 'shall', 'follow', 'title', 'abstract', 'way', 'are', 'free', 'dinner', 'cc']\nemail 11\n['see', 'can', 'rearrange', 'seminar', 'change', 'date', 'visit', 'campus', 'coordinator', 'is', 'very', 'efficient', 'got', 'faculty', 'room', 'presentation', 'morning', 'cc']\nemail 12\n['not', 'getting', 'back', 'earlier', 'is', 'still', 'open', 'can', 'make', 'presentation', 'day', 'cc']\nemail 13\n['mentioned', 'so', 'reserved', 'week', 'is', 'still', 'open', 'email', 'oren', 'phone', 'original', 'message']\nemail 14\n['invitation', 'speak', 'split', 'presentation', 'devote', 'time', 'program', 'plan', 'make', 'presentation', 'energy', 'development', 'valuation', 'challenges', 'role', 'developing', 'forward', 'natural', 'gas', 'electricity', 'shall', 'send', 'bullet', 'few', 'cc']\nemail 15\n['send', 'title', 'talk', 'title', 'etc', 'abstract', 'talk', 'will', 'have', 'hour', 'email', 'oren', 'phone', 'original', 'message', 'pm']\nemail 16\n['not', 'getting', 'back', 'earlier', 'is', 'still', 'open', 'can', 'make', 'presentation', 'day', 'cc']\nemail 17\n['hope', 'remember', 'visited', 'together', 'good', 'friend', 'mine', 'few', 'ago', 'am', 'currently', 'responsible', 'other', 'recruiting', 'finance', 'andor', 'technical', 'would', 'be', 'glad', 'give', 'call', 'talk', 'more', 'program', 'colleague', 'would', 'join', 'well', 'am', 'sending', 'copy', 'brochure', 'email']\n"
     ]
    }
   ],
   "source": [
    "for a in range(len(df)):\n",
    "    print(\"email\", a)\n",
    "    print(df['CleanBody2'][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vectorizer takes tokenized text as input\n",
    "# try with and without stop words \n",
    "#result in more garbage words, if stopwords not taken out\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X=vectorizer.fit_transform(df['CleanBody2'][:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KMeans(max_iter=500, n_clusters=5, n_init=20)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=500, n_init=20)\n",
    "model.fit(X)"
   ]
  },
  {
   "source": [
    "## Hyperparameter choice for Kmeans\n",
    "\n",
    "a true k of 5 was chosen as there are around 18 emails in the dataset and the idea was to use approximately 4 emails to define each cluster\n",
    "\n",
    "'kmeans++' was chosen to converge at result faster (according to sklearn documentation) \n",
    "\n",
    "max_iter of 500 was chosen to in an attempt to achieve the best clustering of words. The default is 300\n",
    "\n",
    "n_init of 20 was chosen for the same reasons and the default here for this is 10"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top terms per cluster:\nCluster 0: cc\n getting\n day\n forwarded\n open\n earlier\n make\n presentation\n agree\n solution\nCluster 1: program\n seminar\n email\n sending\n join\n copy\n remember\n recruiting\n hope\n currently\nCluster 2: shall\n role\n gas\n forward\n valuation\n natural\n energy\n electricity\n development\n developing\nCluster 3: phone\n original\n oren\n email\n message\n talk\n week\n title\n pm\n send\nCluster 4: date\n campus\n change\n room\n visit\n coordinator\n efficient\n got\n rearrange\n morning\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "source": [
    "# Conclusion:\n",
    "\n",
    "TFIDF is not a good way to understand business processes because it depends on the frequency of the words used in the email, not necessarily how they are used.\n",
    "\n",
    "Kmeans only builds off of TFIDF and therefore is not particularly helpful in providing meaningful word clusters for my purpose\n",
    "\n",
    "Junk words like 'oren' (which is the name of a person) and 'cc' which is a field from the email metadata (b/c of nested raw string email threads in the body) did not get filtered out through chunking of POS tags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}